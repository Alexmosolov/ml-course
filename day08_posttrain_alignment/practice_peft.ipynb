{
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6ce8a0fd71364219a4e419783625644a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3bbd59d6c6104d1393bb5af19108d1e7",
              "IPY_MODEL_3518590fb31b4a42a6f60ba79f3aa36d",
              "IPY_MODEL_59ee9c3d21684aba88f56a2666b7c2e9"
            ],
            "layout": "IPY_MODEL_5603af5dc61c4055bf6003989bb151d0"
          }
        },
        "3bbd59d6c6104d1393bb5af19108d1e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d829dad10f749feba950da98705544a",
            "placeholder": "​",
            "style": "IPY_MODEL_e104b1ed9f3848deb229c297176b1582",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "3518590fb31b4a42a6f60ba79f3aa36d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c19330d03d4467aa079a1ea1b49dee4",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2195d2d0a50948cf9bc80c42a66a6fc8",
            "value": 33
          }
        },
        "59ee9c3d21684aba88f56a2666b7c2e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc86bab5b122449490c6a04dc290df64",
            "placeholder": "​",
            "style": "IPY_MODEL_d0e9dd428e244f3a977657d7ccd15bb8",
            "value": " 33/33 [01:45&lt;00:00,  3.44s/it]"
          }
        },
        "5603af5dc61c4055bf6003989bb151d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d829dad10f749feba950da98705544a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e104b1ed9f3848deb229c297176b1582": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c19330d03d4467aa079a1ea1b49dee4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2195d2d0a50948cf9bc80c42a66a6fc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc86bab5b122449490c6a04dc290df64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0e9dd428e244f3a977657d7ccd15bb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10d9034d95754884b5a385007feb6a3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_26715625ebd243b48d40c60233d510bd",
              "IPY_MODEL_404cbfb0280d46f197ab889d19f0bc21",
              "IPY_MODEL_a21bf0a40b5a4ddda9b44c22e59a72fb"
            ],
            "layout": "IPY_MODEL_b5949bbd5e414247be44b29ba1ec8337"
          }
        },
        "26715625ebd243b48d40c60233d510bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55a230e67f3142cbac2083190757a320",
            "placeholder": "​",
            "style": "IPY_MODEL_dc4fcf0403bf41aa9e1b8f15a61681c1",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "404cbfb0280d46f197ab889d19f0bc21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ed05806b12840a2962bf7de2e7ad2fe",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3c95f8677a34a29bf10bc6eb333fe21",
            "value": 33
          }
        },
        "a21bf0a40b5a4ddda9b44c22e59a72fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab61511881e34795b6a77941eb85b735",
            "placeholder": "​",
            "style": "IPY_MODEL_50af6c6b34594704b89217a676dba5e3",
            "value": " 33/33 [01:55&lt;00:00,  3.69s/it]"
          }
        },
        "b5949bbd5e414247be44b29ba1ec8337": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55a230e67f3142cbac2083190757a320": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc4fcf0403bf41aa9e1b8f15a61681c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ed05806b12840a2962bf7de2e7ad2fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3c95f8677a34a29bf10bc6eb333fe21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ab61511881e34795b6a77941eb85b735": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50af6c6b34594704b89217a676dba5e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30588,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Practice: Parameter Efficient Fine-Tuning\n",
        "In this notebook, you're gonna fine-tune large language models within limited GPU memory."
      ],
      "metadata": {
        "id": "aSWEcS2XKgzi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# https://clc.li/oUqUX"
      ],
      "metadata": {
        "id": "-nRH_GfhJv3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --quiet transformers==4.34.1 accelerate==0.24.0 sentencepiece==0.1.99 optimum==1.13.2 peft==0.5.0 bitsandbytes==0.41.2.post2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import transformers\n",
        "from tqdm.auto import tqdm, trange\n",
        "assert torch.cuda.is_available(), \"you need cuda for this part\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "7xeRF_hSKgzs",
        "execution": {
          "iopub.status.busy": "2023-11-21T19:06:49.922449Z",
          "iopub.execute_input": "2023-11-21T19:06:49.923166Z",
          "iopub.status.idle": "2023-11-21T19:06:49.930489Z",
          "shell.execute_reply.started": "2023-11-21T19:06:49.923115Z",
          "shell.execute_reply": "2023-11-21T19:06:49.929463Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee328311-43f1-40b1-be35-2b1ad937370d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.0/301.0 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m112.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m111.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m114.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.4/520.4 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for optimum (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Enoch/llama-7b-hf'\n",
        "\n",
        "# loading Llama tokenizer ...\n",
        "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# ... and the model itself\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, device_map='auto', low_cpu_mem_usage=True, offload_state_dict=True,\n",
        "    load_in_4bit=True, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n",
        ")\n",
        "for param in model.parameters():\n",
        "    param.requires_grad=False\n",
        "\n",
        "model.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\n",
        "model.enable_input_require_grads()     # override an implementation quirk in gradient checkpoints that disables backprop unless inputs require grad\n",
        "# more on gradient checkpointing: https://pytorch.org/docs/stable/checkpoint.html https://arxiv.org/abs/1604.06174"
      ],
      "metadata": {
        "id": "VMzFwx29Kgzu",
        "outputId": "a2627aaa-a871-4902-8a8b-47ff0ee95dde",
        "execution": {
          "iopub.status.busy": "2023-11-21T19:32:56.598575Z",
          "iopub.execute_input": "2023-11-21T19:32:56.599287Z",
          "iopub.status.idle": "2023-11-21T19:33:45.778282Z",
          "shell.execute_reply.started": "2023-11-21T19:32:56.599254Z",
          "shell.execute_reply": "2023-11-21T19:33:45.777431Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6ce8a0fd71364219a4e419783625644a",
            "3bbd59d6c6104d1393bb5af19108d1e7",
            "3518590fb31b4a42a6f60ba79f3aa36d",
            "59ee9c3d21684aba88f56a2666b7c2e9",
            "5603af5dc61c4055bf6003989bb151d0",
            "3d829dad10f749feba950da98705544a",
            "e104b1ed9f3848deb229c297176b1582",
            "7c19330d03d4467aa079a1ea1b49dee4",
            "2195d2d0a50948cf9bc80c42a66a6fc8",
            "cc86bab5b122449490c6a04dc290df64",
            "d0e9dd428e244f3a977657d7ccd15bb8"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ce8a0fd71364219a4e419783625644a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt tuning: the story of a fox (2 pts)\n",
        "\n",
        "![img](https://i.imgur.com/Ux3qQAu.png) (source: theodd1souts.fandom.com)"
      ],
      "metadata": {
        "id": "rgspB2JwSIS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'A quick brown fox'\n",
        "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "\n",
        "for i in range(10):\n",
        "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
        "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
        "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
        "\n",
        "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))"
      ],
      "metadata": {
        "id": "H13pYFRxQi4U",
        "outputId": "597e1af9-399a-41ab-8d8d-9c1c216d906c",
        "execution": {
          "iopub.status.busy": "2023-11-21T11:00:04.024976Z",
          "iopub.execute_input": "2023-11-21T11:00:04.025368Z",
          "iopub.status.idle": "2023-11-21T11:00:19.801464Z",
          "shell.execute_reply.started": "2023-11-21T11:00:04.025329Z",
          "shell.execute_reply": "2023-11-21T11:00:19.800370Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\nOutput: <s>A quick brown fox jumps over the lazy dog.\nA quick\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What a blatant lie! This particular fox assures you that it didn't in fact jump over the lazy dog. No, sir! The fox was just minding its own business. __Your task is to train the model to say truth: no dog was jumped over today.__"
      ],
      "metadata": {
        "id": "VVhZACT6SgLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
        "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "outputs = model(**batch)\n",
        "\n",
        "next_word_logits = outputs.logits[:, :-1]\n",
        "true_next_tokens = batch['input_ids'][:, 1:]\n",
        "loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
        "\n",
        "print(\"Loss:\", loss)"
      ],
      "metadata": {
        "id": "_r6UVDl4NEua",
        "outputId": "67ab27e0-af96-41c7-f0a9-db92e842cd80",
        "execution": {
          "iopub.status.busy": "2023-11-21T11:00:37.915098Z",
          "iopub.execute_input": "2023-11-21T11:00:37.916376Z",
          "iopub.status.idle": "2023-11-21T11:00:38.178593Z",
          "shell.execute_reply.started": "2023-11-21T11:00:37.916343Z",
          "shell.execute_reply": "2023-11-21T11:00:38.177510Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Loss: tensor(3.0725, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Except, we can't train the entire model - that would be 28GB gradients in float32. Instead, let's run [prompt tuning](https://arxiv.org/abs/2104.08691).\n",
        "\n",
        "![img](https://i.imgur.com/VwNNKnb.png)\n"
      ],
      "metadata": {
        "id": "amvNufS8WXa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WordEmbeddingsWithLearnedPrompts(nn.Module):\n",
        "    \"\"\"\n",
        "    To perform prompt tuning, you will need to replace model's original word embeddings with a layer - THIS layer\n",
        "     - that inserts trainable prompts instead of the first N token embeddings. \"\"\"\n",
        "\n",
        "    def __init__(self, word_embeddings: nn.Embedding, num_prompts: int):\n",
        "        super().__init__()\n",
        "        self.original_word_embeddings = word_embeddings\n",
        "        self.num_prompts = num_prompts\n",
        "        self.learnable_prompts = nn.Parameter(\n",
        "            torch.randn(1, num_prompts, word_embeddings.embedding_dim), requires_grad=True)\n",
        "\n",
        "    def forward(self, input_ids: torch.LongTensor):\n",
        "        # input_ids shape: [batch_size, seq length]\n",
        "        assert input_ids.dtype == torch.int64\n",
        "        assert input_ids.shape[1] > self.num_prompts\n",
        "        assert torch.all(input_ids[:, :self.num_prompts] == tokenizer.pad_token_id).item(), \"don't forget to prepend several BOS tokens to input_ids\"\n",
        "\n",
        "        # Your task: embed input_ids, but replace the first :num_prompts: tokens with self.learnable_prompts\n",
        "        # This is because we will prepend :num_prompts: padding tokens at the beginning\n",
        "\n",
        "        # After you are done, you must produce a word embedding vector for each token in input_ids,\n",
        "        # except that the first :num_prompts: vectors should equal learnable_prompts;\n",
        "        # any additional vectors after first :num_prompts: ones should be embedded as usual\n",
        "        # Note: since you're dealing with trainable params, please torch.cat instead of item assignment\n",
        "\n",
        "        embeddings = self.original_word_embeddings(input_ids[:, self.num_prompts:])\n",
        "        output = torch.cat((self.learnable_prompts, embeddings), dim=1)\n",
        "        return output"
      ],
      "metadata": {
        "id": "73ZOCFRZWR98",
        "execution": {
          "iopub.status.busy": "2023-11-21T19:19:27.834996Z",
          "iopub.execute_input": "2023-11-21T19:19:27.835379Z",
          "iopub.status.idle": "2023-11-21T19:19:27.844612Z",
          "shell.execute_reply.started": "2023-11-21T19:19:27.835352Z",
          "shell.execute_reply": "2023-11-21T19:19:27.843585Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_prompts = 16\n",
        "test_emb_layer = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\n",
        "test_input_ids = tokenizer(\"a cat say on a may\", return_tensors='pt')['input_ids'].to(device)\n",
        "\n",
        "space_for_prompts = torch.full([len(test_input_ids), num_prompts], fill_value=tokenizer.pad_token_id,\n",
        "                               dtype=torch.int64, device=device)\n",
        "test_inputs_with_prompts = torch.cat([space_for_prompts, test_input_ids], dim=1)\n",
        "\n",
        "with torch.cuda.amp.autocast():\n",
        "  test_prompt_embeddings = test_emb_layer(test_inputs_with_prompts)\n",
        "\n",
        "assert test_prompt_embeddings.shape[:2] == test_inputs_with_prompts.shape\n",
        "assert test_prompt_embeddings.shape[-1] == model.config.hidden_size\n",
        "assert torch.allclose(test_prompt_embeddings[:, :num_prompts], test_emb_layer.learnable_prompts.float())\n",
        "assert torch.allclose(test_prompt_embeddings[:, num_prompts:], model.model.embed_tokens(test_input_ids).float())\n",
        "print(\"Looks legit!\")"
      ],
      "metadata": {
        "id": "kxUyUU2uT2f1",
        "outputId": "9d0de5c1-162a-4a6f-92c1-1a7f41069464",
        "execution": {
          "iopub.status.busy": "2023-11-21T11:12:19.163853Z",
          "iopub.execute_input": "2023-11-21T11:12:19.164867Z",
          "iopub.status.idle": "2023-11-21T11:12:19.178103Z",
          "shell.execute_reply.started": "2023-11-21T11:12:19.164828Z",
          "shell.execute_reply": "2023-11-21T11:12:19.177063Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Looks legit!\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Now that it works,__ let's inject learnable prompts into the main model and teach it about foxes."
      ],
      "metadata": {
        "id": "FbKPgfT-crqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert isinstance(model.model.embed_tokens, nn.Embedding), \"you have already replaced the embedding layer. If the replacement is broken, please reload the model\"\n",
        "\n",
        "model.model.embed_tokens = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\n",
        "\n",
        "opt = torch.optim.Adam([model.model.embed_tokens.learnable_prompts], lr=0.01)"
      ],
      "metadata": {
        "id": "QRe0lpREV49G",
        "execution": {
          "iopub.status.busy": "2023-11-21T11:12:22.270705Z",
          "iopub.execute_input": "2023-11-21T11:12:22.271101Z",
          "iopub.status.idle": "2023-11-21T11:12:22.277569Z",
          "shell.execute_reply.started": "2023-11-21T11:12:22.271071Z",
          "shell.execute_reply": "2023-11-21T11:12:22.276654Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
        "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "space_for_prompts = torch.full([len(test_input_ids), num_prompts], fill_value=tokenizer.pad_token_id,\n",
        "                               dtype=torch.int64, device=device)\n",
        "batch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\n",
        "batch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n",
        "\n",
        "outputs = model(**batch)\n",
        "next_word_logits = outputs.logits[:, num_prompts : -1, :]\n",
        "true_next_tokens = batch['input_ids'][:, num_prompts + 1:]\n",
        "loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
        "print(\"Loss:\", loss)\n",
        "\n",
        "\n",
        "for i in range(100):\n",
        "    opt.zero_grad()\n",
        "    outputs = model(**batch)\n",
        "    next_word_logits = outputs.logits[:, num_prompts : -1, :]\n",
        "    loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "assert loss.item() <= 0.1\n",
        "print(\"Good job!\")"
      ],
      "metadata": {
        "id": "3gVQzgdka-Bm",
        "execution": {
          "iopub.status.busy": "2023-11-21T11:12:24.846903Z",
          "iopub.execute_input": "2023-11-21T11:12:24.847762Z",
          "iopub.status.idle": "2023-11-21T11:13:26.464828Z",
          "shell.execute_reply.started": "2023-11-21T11:12:24.847729Z",
          "shell.execute_reply": "2023-11-21T11:13:26.463813Z"
        },
        "trusted": true,
        "outputId": "fd3075a1-4a7f-43cd-dd5b-42936d251b0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Loss: tensor(7.4344, device='cuda:0', grad_fn=<NllLossBackward0>)\nGood job!\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'A quick brown fox'\n",
        "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "batch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\n",
        "batch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n",
        "\n",
        "\n",
        "for i in range(15):\n",
        "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
        "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
        "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
        "\n",
        "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0, num_prompts:].cpu().numpy().tolist()))\n",
        "\n",
        "# if you did everything right, the model will deny that the fox jumped over the lazy dog"
      ],
      "metadata": {
        "id": "F7DkWHD-r1Xo",
        "execution": {
          "iopub.status.busy": "2023-11-21T11:13:32.050463Z",
          "iopub.execute_input": "2023-11-21T11:13:32.050862Z",
          "iopub.status.idle": "2023-11-21T11:13:36.010400Z",
          "shell.execute_reply.started": "2023-11-21T11:13:32.050831Z",
          "shell.execute_reply": "2023-11-21T11:13:36.009396Z"
        },
        "trusted": true,
        "outputId": "eeeb812b-01b4-4145-8fda-e10aacb9fa6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\nOutput: <s>A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using HuggingFace PEFT (2 points)\n",
        "\n",
        "[`peft`](https://huggingface.co/docs/peft/index) is a transformer's sister library that allows you to apply various __p__arameter __e__fficient __f__ine-__t__uning methods to pre-trained transformers. The library imlements both prompt tuning, prefix tuning, as well as several adapter-based techniques under a common interface:\n",
        "\n"
      ],
      "metadata": {
        "id": "sEkoFNdlshv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import peft\n",
        "assert isinstance(model.model.embed_tokens, nn.Embedding), \"please reload the model\"\n",
        "\n",
        "peft_config = peft.PromptTuningConfig(task_type=peft.TaskType.CAUSAL_LM, num_virtual_tokens=16)\n",
        "model = peft.get_peft_model(model, peft_config)  # note: for most peft methods, this line also modifies model in-place\n",
        "print(\"Trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "print(\"Total parameters (excluding quantization):\", sum(p.numel() for p in model.parameters()))"
      ],
      "metadata": {
        "id": "mqEEpZm2Q4UC",
        "outputId": "2b760d0e-ac1e-4580-9472-997d1275385d",
        "execution": {
          "iopub.status.busy": "2023-11-21T19:33:45.779856Z",
          "iopub.execute_input": "2023-11-21T19:33:45.780149Z",
          "iopub.status.idle": "2023-11-21T19:33:45.801418Z",
          "shell.execute_reply.started": "2023-11-21T19:33:45.780123Z",
          "shell.execute_reply": "2023-11-21T19:33:45.800573Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Trainable parameters: 65536\nTotal parameters (excluding quantization): 3500478464\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Your task: optimize the PEFT-wrapped model to achieve next token prediction loss < 0.1, but this time using PEFT\n",
        "# Please note: you no longer need to prepend PAD tokens, but you still need to skip :num_virtual_tokens: first logits.\n",
        "# Finally, generate the sentence to make sure that the model learned the truth."
      ],
      "metadata": {
        "id": "UW54GnzCwVpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to structure your code as you see fit - as long as it's legible :)"
      ],
      "metadata": {
        "id": "71vJ9Mq7w67f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
        "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-21T19:33:49.827965Z",
          "iopub.execute_input": "2023-11-21T19:33:49.828594Z",
          "iopub.status.idle": "2023-11-21T19:33:49.834302Z",
          "shell.execute_reply.started": "2023-11-21T19:33:49.828558Z",
          "shell.execute_reply": "2023-11-21T19:33:49.833348Z"
        },
        "trusted": true,
        "id": "VtAV0oqYJUHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.Adam(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-21T19:33:52.450797Z",
          "iopub.execute_input": "2023-11-21T19:33:52.451165Z",
          "iopub.status.idle": "2023-11-21T19:33:52.458763Z",
          "shell.execute_reply.started": "2023-11-21T19:33:52.451136Z",
          "shell.execute_reply": "2023-11-21T19:33:52.457792Z"
        },
        "trusted": true,
        "id": "v1ttJsXdJUHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "    opt.zero_grad()\n",
        "    outputs = model(**batch)\n",
        "    next_word_logits = outputs.logits[:, num_prompts : -1, :]\n",
        "    loss = F.cross_entropy(next_word_logits.flatten(0, 1), batch['input_ids'][:, 1:].flatten(0, 1))\n",
        "    print(loss)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    if loss.item() < 0.1:\n",
        "        break"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-21T19:33:55.065848Z",
          "iopub.execute_input": "2023-11-21T19:33:55.066193Z",
          "iopub.status.idle": "2023-11-21T19:34:22.884386Z",
          "shell.execute_reply.started": "2023-11-21T19:33:55.066167Z",
          "shell.execute_reply": "2023-11-21T19:34:22.883414Z"
        },
        "trusted": true,
        "id": "fLDTeHlkJUHV",
        "outputId": "e14f3884-b36c-4fd0-8611-ac6f92bcb02a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "tensor(7.7071, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(6.9944, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(6.4791, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(6.0682, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(5.7262, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(5.4201, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(5.1351, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(4.8612, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(4.5962, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(4.3479, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(4.1233, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(3.9166, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(3.7192, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(3.5291, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(3.3444, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(3.1616, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(2.9778, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(2.7930, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(2.6105, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(2.4351, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(2.2672, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(2.1032, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(1.9413, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(1.7826, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(1.6317, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(1.4896, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(1.3525, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(1.2193, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(1.0925, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(0.9737, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(0.8610, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(0.7509, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(0.6428, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(0.5416, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(0.4512, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(0.3709, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(0.3009, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(0.2442, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(0.2013, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(0.1696, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(0.1463, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(0.1286, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(0.1143, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(0.1019, device='cuda:0', grad_fn=<NllLossBackward0>)\ntensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'A quick brown fox'\n",
        "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "\n",
        "for i in range(15):\n",
        "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
        "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
        "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
        "\n",
        "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-21T19:36:00.132106Z",
          "iopub.execute_input": "2023-11-21T19:36:00.132484Z",
          "iopub.status.idle": "2023-11-21T19:36:04.074464Z",
          "shell.execute_reply.started": "2023-11-21T19:36:00.132452Z",
          "shell.execute_reply": "2023-11-21T19:36:04.073465Z"
        },
        "trusted": true,
        "id": "_XSlAG_uJUHW",
        "outputId": "73249213-dbf8-4173-88af-e2b7b928b360"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\nOutput: <s>A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameter-efficient finetuning with LoRA (2 points)\n",
        "\n",
        "When training on more serious tasks, you can use low-rank adapters based on the [LoRA paper](https://arxiv.org/pdf/2106.09685.pdf).\n",
        "\n",
        "The core idea is to add low-rank adapters __in parallel with existing linear layers,__ like this:\n",
        "<center><img src=\"https://i.imgur.com/6bQLNiG.png\" width=240px></center>\n",
        "\n",
        "In the original LoRA paper, the adapters were only added to attention projection matrices. However, [subsequent works](https://arxiv.org/abs/2305.14314) show that it is useful to adapt FFNs as well. But before we do any training, we need to implement the basic LoRA layer."
      ],
      "metadata": {
        "id": "uCkpKYjWxfhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# re-load the model to remove any previous PEFT tuners\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, device_map='auto', low_cpu_mem_usage=True, offload_state_dict=True,\n",
        "    load_in_4bit=True, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n",
        ")\n",
        "for param in model.parameters():\n",
        "    param.requires_grad=False\n",
        "model.gradient_checkpointing_enable()\n",
        "model.enable_input_require_grads()"
      ],
      "metadata": {
        "id": "8zundaSzx90r",
        "outputId": "3faf7150-7685-4089-cf58-e03e4fce8bc6",
        "execution": {
          "iopub.status.busy": "2023-11-21T11:26:03.067333Z",
          "iopub.execute_input": "2023-11-21T11:26:03.068141Z",
          "iopub.status.idle": "2023-11-21T11:26:49.843235Z",
          "shell.execute_reply.started": "2023-11-21T11:26:03.068108Z",
          "shell.execute_reply": "2023-11-21T11:26:49.842426Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "2fb27703f1894d53b34b89cebeaf8265"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2fb27703f1894d53b34b89cebeaf8265"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LoRALayer(nn.Module):\n",
        "    \"\"\"Wraps a linear layer with LoRA-like adapter. Wraps an existing OPT linear layer\"\"\"\n",
        "    def __init__(self, module: nn.Linear, rank: int):\n",
        "        super().__init__()\n",
        "        self.module = module  # pre-trained (frozen) linear layer\n",
        "        self.adapter_A = nn.Parameter(torch.empty(module.in_features, rank, device=module.weight.device))\n",
        "        nn.init.kaiming_uniform_(self.adapter_A, a=5 ** 0.5)\n",
        "        self.adapter_B = nn.Parameter(torch.zeros(rank, module.out_features, device=module.weight.device))\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Apply self.module and LoRA adapter, return the sum (self.module outputs + adapter outputs)\n",
        "        return self.module(input) + torch.matmul(input, torch.matmul(self.adapter_A, self.adapter_B))"
      ],
      "metadata": {
        "id": "MJ_hq4fwyPVR",
        "execution": {
          "iopub.status.busy": "2023-11-21T14:57:06.451023Z",
          "iopub.execute_input": "2023-11-21T14:57:06.451475Z",
          "iopub.status.idle": "2023-11-21T14:57:06.459108Z",
          "shell.execute_reply.started": "2023-11-21T14:57:06.451431Z",
          "shell.execute_reply": "2023-11-21T14:57:06.458248Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test your implementation\n",
        "test_linear = nn.Linear(128, 128)\n",
        "test_linear.weight.data[...] = torch.eye(128)\n",
        "test_adapter = LoRALayer(test_linear, rank=8)\n",
        "\n",
        "assert torch.allclose(test_adapter(torch.ones(1, 1, 128)), test_linear.bias + 1), \"please check your forward pass\"\n",
        "\n",
        "test_adapter.adapter_A.data[...] = torch.linspace(0.1, -0.5, 128 * 8).view(128, 8)\n",
        "test_adapter.adapter_B.data[...] = torch.linspace(0.5, -0.1, 128 * 8).view(8, 128)\n",
        "test_linear.bias.data[...] = torch.linspace(1., -1., 128)\n",
        "\n",
        "dummy_loss = F.mse_loss(test_adapter(torch.ones(1, 128) / 128).squeeze(), torch.linspace(-1, 1, 128))\n",
        "assert torch.allclose(dummy_loss, torch.tensor(1.3711389), rtol=0, atol=1e-4)\n",
        "dummy_loss.backward()\n",
        "assert all(w.grad is not None for w in [test_adapter.adapter_A, test_adapter.adapter_B]), \"some adapter weights have no grad\"\n",
        "assert torch.allclose(test_adapter.adapter_A.grad.sum(), torch.tensor(-0.60158), rtol=0, atol=1e-4), \"bad grad w.r.t. A\"\n",
        "assert torch.allclose(test_adapter.adapter_B.grad.sum(), torch.tensor(0.9931), rtol=0, atol=1e-4), \"bad grad w.r.t. B\"\n",
        "# note: bad grad means that your code is different from LoRA paper OR that your code is not autograd-friendly (e.g. no_grad)\n",
        "del dummy_loss, test_linear, test_adapter\n",
        "print(\"All tests passed!\")"
      ],
      "metadata": {
        "id": "tTzOs65JydcS",
        "outputId": "e07177c9-2f2b-432a-8a97-9e507df166bf",
        "execution": {
          "iopub.status.busy": "2023-11-21T11:31:48.384004Z",
          "iopub.execute_input": "2023-11-21T11:31:48.384764Z",
          "iopub.status.idle": "2023-11-21T11:31:48.455428Z",
          "shell.execute_reply.started": "2023-11-21T11:31:48.384730Z",
          "shell.execute_reply": "2023-11-21T11:31:48.454556Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "All tests passed!\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply LoRA to the model\n",
        "\n",
        "The code below applies LoRA adapters on top of Q/K/V linear layers in Llama attention. You may also choose to modify other layers:\n",
        "* self_attn.o_proj - attention output projection\n",
        "* mlp.up_proj, mlp.gate_proj, mlp.down_proj - transformer feedforward layers\n",
        "* lm_head - output LM head\n",
        "\n",
        "__Note:__ please scroll down for the homework task"
      ],
      "metadata": {
        "id": "tajVTsvLulB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_rank = 8\n",
        "\n",
        "for name, module in model.model.layers.named_modules():\n",
        "    if 'LlamaDecoderLayer' in repr(type(module)):\n",
        "        module.self_attn.q_proj = LoRALayer(module.self_attn.q_proj, rank=lora_rank).to(device)\n",
        "        module.self_attn.k_proj = LoRALayer(module.self_attn.k_proj, rank=lora_rank).to(device)\n",
        "        module.self_attn.v_proj = LoRALayer(module.self_attn.v_proj, rank=lora_rank).to(device)\n",
        "\n",
        "assert sum(isinstance(module, LoRALayer) for module in model.modules()) == 96  # for Llama-7B"
      ],
      "metadata": {
        "id": "davyUVEwulB6",
        "execution": {
          "iopub.status.busy": "2023-11-21T11:31:52.056530Z",
          "iopub.execute_input": "2023-11-21T11:31:52.056929Z",
          "iopub.status.idle": "2023-11-21T11:31:52.093811Z",
          "shell.execute_reply.started": "2023-11-21T11:31:52.056898Z",
          "shell.execute_reply": "2023-11-21T11:31:52.092807Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = tokenizer(\"This model wants to share its greatest secret:\", return_tensors='pt', return_token_type_ids=False)\n",
        "# test a single training step, make sure we get meaningful gradients\n",
        "with torch.cuda.amp.autocast(dtype=torch.float32):\n",
        "    out = model.forward(**batch)\n",
        "    (out.logits.norm() / 100).backward()\n",
        "\n",
        "for i, module in enumerate(model.modules()):\n",
        "    if isinstance(module, LoRALayer):\n",
        "        assert module.adapter_B.grad is not None\n",
        "        assert module.adapter_B.grad.norm().item() > 0\n",
        "\n",
        "model.zero_grad(set_to_none=True)\n",
        "print(\"Grad check successful, well done!\")"
      ],
      "metadata": {
        "tags": [],
        "id": "AWzfvc0EulB6",
        "outputId": "b432afe7-08b9-4cb2-c6ac-01f85352a689",
        "execution": {
          "iopub.status.busy": "2023-11-21T11:31:55.717721Z",
          "iopub.execute_input": "2023-11-21T11:31:55.718064Z",
          "iopub.status.idle": "2023-11-21T11:32:10.212039Z",
          "shell.execute_reply.started": "2023-11-21T11:31:55.718037Z",
          "shell.execute_reply": "2023-11-21T11:32:10.211054Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Grad check successful, well done!\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (example) How to train your model\n",
        "\n",
        "The example below shows how to train the LoRA adapters on a dummy dataset. You will need to run a _similar_ training task later.\n",
        "\n",
        "__Note:__ please scroll down for the homework task"
      ],
      "metadata": {
        "id": "rjIJ1vkUulB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking if the model can learn. Change max_steps for proper training\n",
        "import datasets\n",
        "data = datasets.load_dataset(\"Abirate/english_quotes\", split=\"train[:32]\") # 32 lines\n",
        "data = data.map(lambda samples: tokenizer(samples['quote']), batched=True)\n",
        "model._hf_peft_config_loaded = True  # silence a warning from HF trainer\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model, train_dataset=data,\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=2, gradient_accumulation_steps=1,\n",
        "        # note: if you want larger batch size, increase gradient_accumulation_steps\n",
        "        warmup_steps=250, max_steps=100, learning_rate=2e-4, fp16=True,\n",
        "        logging_steps=1, output_dir='outputs', report_to=None),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "# if you see cache warnings, set `model.config.use_cache = False` to silence them. Please re-enable for inference!\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# NOTE: this is just an example! you do not have to wait for this progressbar to finish :)"
      ],
      "metadata": {
        "id": "r9mIpntHulB8",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final task: *actually* train the model (4 points)\n",
        "\n",
        "Your task is to fine-tune the model to _generate python code_. Please use the above examples for inspiration. More specifically,\n",
        "\n",
        "* __dataset:__ use [codeparrot-clean](https://huggingface.co/datasets/codeparrot/codeparrot-clean) or any other data containing python code. Since you do not need much data for this excercise, it is enough to use just shorter validation subset of `codeparrots`\n",
        "* __preprocessing:__ select python code based on file extentions (.py)  (may skip in case of codeparrot - it is 100% python)\n",
        "* __short lines:__ please take the first 512 characters of each line\n",
        "* __adapter type:__ please use LoRA as defined above __plus at least one of:__\n",
        "   - extra adapter on lm_head\n",
        "   - extra adapter on MLP components (mlp.*)\n",
        "   - trainable input embeddings (requires tweaking memory usage)\n",
        "\n",
        "* __training:__ you do not have to train to convergence. If all goes well, your model should `.generate` code after 500 steps. Please use batch size of at least 4 (4 x 1 x 512 tokens) using `gradient_accumulation_steps=4`.\n",
        "\n",
        "\n",
        "Note: the peft library also has LoRA implementation. However, we ask that for this assignment you show at least one complete training run with your own LoRA code.\n",
        "\n",
        "__Alternative assignment:__ Instead of doing python code, feel free to substitute the task with any other dataset, e.g. your favorite artist or podcast, as long as it's ethical. If you choose your own task, please show examples of what your model learned - or did not learn, akin to the code examples below."
      ],
      "metadata": {
        "id": "DQUlqoEAulB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, device_map='auto', low_cpu_mem_usage=True, offload_state_dict=True,\n",
        "    load_in_4bit=True, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n",
        ")\n",
        "for param in model.parameters():\n",
        "    param.requires_grad=False\n",
        "model.gradient_checkpointing_enable()\n",
        "model.enable_input_require_grads()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-21T15:32:31.751451Z",
          "iopub.execute_input": "2023-11-21T15:32:31.752411Z",
          "iopub.status.idle": "2023-11-21T15:33:48.591549Z",
          "shell.execute_reply.started": "2023-11-21T15:32:31.752371Z",
          "shell.execute_reply": "2023-11-21T15:33:48.590386Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "10d9034d95754884b5a385007feb6a3a",
            "26715625ebd243b48d40c60233d510bd",
            "404cbfb0280d46f197ab889d19f0bc21",
            "a21bf0a40b5a4ddda9b44c22e59a72fb",
            "b5949bbd5e414247be44b29ba1ec8337",
            "55a230e67f3142cbac2083190757a320",
            "dc4fcf0403bf41aa9e1b8f15a61681c1",
            "0ed05806b12840a2962bf7de2e7ad2fe",
            "f3c95f8677a34a29bf10bc6eb333fe21",
            "ab61511881e34795b6a77941eb85b735",
            "50af6c6b34594704b89217a676dba5e3"
          ]
        },
        "id": "67BWDO0XJUHX",
        "outputId": "8745b21e-c1b9-40e6-b60d-6a7ad0989674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10d9034d95754884b5a385007feb6a3a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompts =  ['', 'import', 'from', 'while', 'try', 'if', 'for', 'torch']  # feel free to add a few more that are not 100% assiciated with Python\n",
        "\n",
        "before_finetuning = []\n",
        "for prompt in prompts:\n",
        "    output_tokens = model.generate(**tokenizer(prompt, return_tensors='pt'),\n",
        "                               do_sample=True, min_length=50, max_length=100)\n",
        "    before_finetuning.append(tokenizer.decode(output_tokens[0].cpu().numpy()))\n",
        "\n",
        "# generate baseline samples with the selected prompts before finetuning\n",
        "# please feel free to use transformers.Trainer (as above) or your custom training code\n",
        "# after the training concludes, please show examples of text generated by your model. It is expected to look like Python code fragments\n",
        "# print the generation examples nicely (suggestion: use pandas or HTML) for easier comparison\n",
        "# note: your LoRA-enhanced model can run generation the same way as the non-trained model (above)"
      ],
      "metadata": {
        "id": "_LfFWSYhulB8",
        "execution": {
          "iopub.status.busy": "2023-11-21T18:56:36.567716Z",
          "iopub.execute_input": "2023-11-21T18:56:36.568102Z",
          "iopub.status.idle": "2023-11-21T18:57:22.804538Z",
          "shell.execute_reply.started": "2023-11-21T18:56:36.568069Z",
          "shell.execute_reply": "2023-11-21T18:57:22.803337Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "602181d3-e4a0-4214-b374-67945c0b3e7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1539: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "table_template = \"\"\"<table style=\"border:1px solid black\" >\n",
        "  <tr>\n",
        "    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n",
        "    <th style=\"text-align: center; border:1px solid black\">BEFORE</th>\n",
        "    <th style=\"text-align: center; border:1px solid black\">AFTER</th>\n",
        "  </tr>\n",
        "{}\n",
        "</table>\"\"\"\n",
        "\n",
        "row_template = '''  <tr>\n",
        "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`{}`</pre></td>\n",
        "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n",
        "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n",
        "  </tr>'''\n",
        "\n",
        "rows = []\n",
        "\n",
        "for i, prompt in enumerate(prompts):\n",
        "    # replace placeholders in the format() arguments\n",
        "    rows.append(row_template.format(prompt, before_finetuning[i][3:], after_finetuning[i][3:]))\n",
        "\n",
        "display(HTML(table_template.format('\\n'.join(rows))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ASrZqpg_Xijp",
        "outputId": "1a2e4d98-af6f-49d5-a07b-9c8b4933b7ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table style=\"border:1px solid black\" >\n",
              "  <tr>\n",
              "    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n",
              "    <th style=\"text-align: center; border:1px solid black\">BEFORE</th>\n",
              "    <th style=\"text-align: center; border:1px solid black\">AFTER</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">``</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">│ ● Author: Troy Brownfield\n",
              "“You can make your own fate.”\n",
              "This is one of my favorite things ever said to me. It sticks out most in my mind because I think back to it when I’s sitting on my bed and trying to convince myself to do something out of this world that I was afraid to do.\n",
              "I was a scrawny high school freshman, and the thought of asking a girl to attend a Friday night football game</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">Multimedia is one of the major components in a modern web design trend. In websites where you would like to make sure that your users are in for not only browsing but also have a good story telling with your website, using multimedia becomes an obvious choice.\n",
              "The good news is that creating a Multimedia site is not as hard and cumbersome as it used to be. Webs is a perfect example on how a non-technical user can use the features offered by its platform</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`import`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">import Foundation\n",
              "\n",
              "// Swift 2.1 only has NSURLConnection, so this class implements NSURLConnectionDelegate so that we still have the ability to send an extension error.\n",
              "/// The `NSURLConnectionDataDelegate` object encapsulates the `NSURLConnection` in a way that exposes delegate methods to the caller.\n",
              "public protocol NSURLConnectionDataDelegate: NSObjectProtocol, NSURLConnectionDownloadDelegate {\n",
              "    var connection: NSURLConnection { get set }</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">import inspect\n",
              "\n",
              "import nose.Case\n",
              "from _pytest.fixture import Nested\n",
              "\n",
              "def _test_to_native(cls):\n",
              "    \"\"\"Test that attributes/methods are converted to native\"\"\"\n",
              "    for name, value in inspect.getmembers(cls):\n",
              "        if inspect.isfunction(value):\n",
              "            yield _function_to_native, name, value\n",
              "        else:\n",
              "            if issubclass(value, inspect):\n",
              "               </pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`from`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">from Cryptopp import Detail\n",
              "from Cryptopp import HP18\n",
              "from Cryptopp import TM18\n",
              "from Cryptopp import TM20\n",
              "from Cryptopp import TM25\n",
              "from Cryptopp import OP1\n",
              "from Cryptopp import OP2\n",
              "from Cryptopp import OP3\n",
              "from Cryptopp import OP4\n",
              "from Cryptopp import OP5\n",
              "from Cryptopp import OP6\n",
              "from Crypt</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">from __future__ import absolute_import, division, print_function\n",
              "try:\n",
              "  from . import _io\n",
              "finally:\n",
              "  pass\n",
              "\n",
              "from . import _io\n",
              "from . import _regression\n",
              "\n",
              "import gc\n",
              "import os\n",
              "\n",
              "\n",
              "class IO(object):\n",
              "\n",
              "  def __init__(self):\n",
              "    self._io = _io\n",
              "    self._regression = _regression\n",
              "\n",
              "  def load(self, filepath</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`while`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">while(true);\n",
              "    if(x==6)return 1;\n",
              "x++;\n",
              "    y++;\n",
              "}\n",
              "\\end{code}\n",
              "\n",
              "Comment: @Steven Well, yes and no.  The second loop is infinite or it will crash eventually (but you can get a stack overflow exception first, or before).  The first loop is finite, and will loop forever if the condition evaluates to `false` while both are running (because `false` is</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">while both of us were away on holiday\n",
              "I wrote a rather long post about this in January last year, if you’re curious.\n",
              "There is an elephant in the room. A very big elephant. And our family is going to see if we can move it or not.\n",
              "I’m not going to name the elephant, because it really is our choice whether we want to take it on, and I want to make sure there’s room</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`try`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">try this link (http://www.wbx.de/index.php?id=cms&page=123750&k=0) I think this is the only site that works at the moment. There should be a working stream at least 2 hours before the match starts.\n",
              "\"VfB Stuttgart wird ab Mittwoch mt den FC Bayern Münchensystem gegenreisenden FC Bayern spielen\"\n",
              "I can't</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">try them online by logging in at the normal time (e.g. 6pm on Friday) and if there are places available you will be sent an automatically generated email to your registration email address on the same evening notifying you of your booking. This will include the instructions for downloading the required files onto your mp3 player in time for your start location on the first day.\n",
              "Please read our terms and conditions page and our data protection policy page.\n",
              "If you still want to</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`if`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">if (Math.abs(currentTime - expectedTime) < allowedDeviation) currentTime = new Time(currentTime)\n",
              "\\end{code}\n",
              "\n",
              "Then in the time method\n",
              "\n",
              "\\begin{code}\n",
              "public Time() {\n",
              "    // This should be the time difference with a clock\n",
              "    this.time = 0;\n",
              "    // Time values from now on should be in the past\n",
              "    this.timeIsRelative = true;\n",
              "}\n",
              "\\</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">if you’re reading this you’re gonna be one\n",
              "of those guys in your 80s and 90s\n",
              "with a great story to tell\n",
              "about the time you heard the Beach Boys play\n",
              "back when they were still cool enough for him to take you to\n",
              "he must have thought\n",
              "that his kids wouldn’t care\n",
              "that no one else would care\n",
              "and the kids grew up to be teenagers\n",
              "but no matter because\n",
              "your</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`for`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"> for 378 E. Main in Fayetteville, Arkansas.\n",
              "Joshua Longoria was hired as the first principal of the new school.\n",
              "His vision for the school is summed up in his mantra:\"Honor the Child, Respect the Parent, Expect the Teacher.\"</s></pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">for all the children I'll never have!\n",
              "Mom's been babysitting for the kids across the street for the entire time we've lived here (going on 3.5 years!). She would get us 2 hours at a time every few weeks or so throughout that time. I've been dying to have a few of those times to take care of my own kiddos. My two brothers have 6 kids each (yes,</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`torch`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">torchbearer said...\n",
              "The U.S. government has given money to groups that do not even want to protect their own people and is giving money to the very ones responsible for their mass killings. How would you feel if it was the Mexican government making these deals? Would you feel the same way?\n",
              "I like to give credence to the notion that the United States did not do everything in it's power to prevent the slaughter of the innocent</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">torchlite.com\n",
              "1985—1992 • 1992—2010 • 2010— Present\n",
              "We design, we produce, we deliver.\n",
              "We’re experienced and creative with what-if’s and “how-can-I’s.”\n",
              "We use sustainable fabrics and materials.\n",
              "And we listen to what has been missing all along — the voices of women</pre></td>\n",
              "  </tr>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"codeparrot/codeparrot-clean-valid\")\n",
        "data = data.filter(lambda sample: sample['path'].endswith(\".py\"))\n",
        "data = data.map(lambda sample: tokenizer(sample['content'], truncation=True, max_length=512),\n",
        "                batched=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "zEE86HH4JUHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_rank = 8\n",
        "\n",
        "for name, module in model.model.layers.named_modules():\n",
        "    if 'LlamaDecoderLayer' in repr(type(module)):\n",
        "        module.self_attn.q_proj = LoRALayer(module.self_attn.q_proj, rank=lora_rank).to(device)\n",
        "        module.self_attn.k_proj = LoRALayer(module.self_attn.k_proj, rank=lora_rank).to(device)\n",
        "        module.self_attn.v_proj = LoRALayer(module.self_attn.v_proj, rank=lora_rank).to(device)\n",
        "\n",
        "    if 'LlamaMLP' in repr(type(module)):\n",
        "        module.gate_proj = LoRALayer(module.gate_proj, rank=lora_rank).to(device)\n",
        "        module.up_proj = LoRALayer(module.up_proj, rank=lora_rank).to(device)\n",
        "        module.down_proj = LoRALayer(module.down_proj, rank=lora_rank).to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-21T15:33:48.593609Z",
          "iopub.execute_input": "2023-11-21T15:33:48.593995Z",
          "iopub.status.idle": "2023-11-21T15:33:48.653191Z",
          "shell.execute_reply.started": "2023-11-21T15:33:48.593957Z",
          "shell.execute_reply": "2023-11-21T15:33:48.651937Z"
        },
        "trusted": true,
        "id": "_BiMikWCJUHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model._hf_peft_config_loaded = True\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model, train_dataset=data['train'],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=4, gradient_accumulation_steps=4, warmup_steps=250,\n",
        "        max_steps=100, learning_rate=2e-4, fp16=True,\n",
        "        logging_steps=1, output_dir='outputs', report_to=None),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-21T15:33:48.654555Z",
          "iopub.execute_input": "2023-11-21T15:33:48.654906Z",
          "iopub.status.idle": "2023-11-21T18:14:01.023500Z",
          "shell.execute_reply.started": "2023-11-21T15:33:48.654873Z",
          "shell.execute_reply": "2023-11-21T18:14:01.022508Z"
        },
        "trusted": true,
        "id": "QGBEwwiDJUHX",
        "outputId": "be7eb35c-7293-4c0b-b7ca-2dd875291ea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 2:38:35, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.014300</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.149300</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.223600</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.977600</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.049900</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.177700</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.084700</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.999800</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.059600</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.989500</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>1.201100</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.177600</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>1.068800</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.119500</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.962300</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>1.118000</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>1.043100</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>1.123000</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>1.168900</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.099200</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>1.257300</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.955900</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.906000</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>1.167000</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>1.138800</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.968100</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>1.101300</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>1.111700</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>1.057900</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.083800</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>1.014600</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>1.164700</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.931500</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>1.104000</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>1.205700</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.965700</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>1.036600</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>1.117700</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>1.136900</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.113800</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>1.130800</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>1.080100</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>1.129800</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>1.171000</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.997400</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>1.129100</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>1.225900</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>1.037400</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.904900</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.054000</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>0.904700</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>1.224300</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>1.109700</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>1.109200</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.932900</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>0.869400</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>1.057000</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>0.953100</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>0.970400</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.136400</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>1.058100</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>0.879500</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>0.877400</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>1.000400</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>0.944900</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>0.836600</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>0.877300</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>1.080100</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>0.849300</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.136500</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>0.904100</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>1.040000</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>1.005100</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>1.035400</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.090300</td>\n    </tr>\n    <tr>\n      <td>76</td>\n      <td>1.154800</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>0.919900</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>1.016600</td>\n    </tr>\n    <tr>\n      <td>79</td>\n      <td>1.049000</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.091300</td>\n    </tr>\n    <tr>\n      <td>81</td>\n      <td>0.979300</td>\n    </tr>\n    <tr>\n      <td>82</td>\n      <td>0.966900</td>\n    </tr>\n    <tr>\n      <td>83</td>\n      <td>1.021200</td>\n    </tr>\n    <tr>\n      <td>84</td>\n      <td>0.872700</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>1.163600</td>\n    </tr>\n    <tr>\n      <td>86</td>\n      <td>0.934000</td>\n    </tr>\n    <tr>\n      <td>87</td>\n      <td>0.868300</td>\n    </tr>\n    <tr>\n      <td>88</td>\n      <td>0.942800</td>\n    </tr>\n    <tr>\n      <td>89</td>\n      <td>1.106500</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.879800</td>\n    </tr>\n    <tr>\n      <td>91</td>\n      <td>1.113900</td>\n    </tr>\n    <tr>\n      <td>92</td>\n      <td>1.074800</td>\n    </tr>\n    <tr>\n      <td>93</td>\n      <td>1.021100</td>\n    </tr>\n    <tr>\n      <td>94</td>\n      <td>1.062800</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>1.220900</td>\n    </tr>\n    <tr>\n      <td>96</td>\n      <td>1.048400</td>\n    </tr>\n    <tr>\n      <td>97</td>\n      <td>0.858900</td>\n    </tr>\n    <tr>\n      <td>98</td>\n      <td>1.012800</td>\n    </tr>\n    <tr>\n      <td>99</td>\n      <td>1.095600</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.114600</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainOutput(global_step=100, training_loss=1.0467948484420777, metrics={'train_runtime': 9611.9271, 'train_samples_per_second': 0.166, 'train_steps_per_second': 0.01, 'total_flos': 3.25643547967488e+16, 'train_loss': 1.0467948484420777, 'epoch': 0.03})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "after_finetuning = []\n",
        "for prompt in prompts:\n",
        "    output_tokens = model.generate(**tokenizer(prompt, return_tensors='pt'),\n",
        "                               do_sample=True, min_length=50, max_length=100)\n",
        "    after_finetuning.append(tokenizer.decode(output_tokens[0].cpu().numpy()))"
      ],
      "metadata": {
        "trusted": true,
        "id": "V2v_X_DaJUHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This template helps to compare generated code samples in pretty table form\n",
        "# feel free to present your work in other forms\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "table_template = \"\"\"<table style=\"border:1px solid black\" >\n",
        "  <tr>\n",
        "    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n",
        "    <th style=\"text-align: center; border:1px solid black\">BEFORE</th>\n",
        "    <th style=\"text-align: center; border:1px solid black\">AFTER</th>\n",
        "  </tr>\n",
        "{}\n",
        "</table>\"\"\"\n",
        "\n",
        "row_template = '''  <tr>\n",
        "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`{}`</pre></td>\n",
        "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n",
        "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n",
        "  </tr>'''\n",
        "\n",
        "rows = []\n",
        "\n",
        "for i, prompt in enumerate(prompts):\n",
        "    # replace placeholders in the format() arguments\n",
        "    rows.append(row_template.format(prompt, before_finetuning[i][3:], after_finetuning[i][3:]))\n",
        "\n",
        "display(HTML(table_template.format('\\n'.join(rows))))"
      ],
      "metadata": {
        "id": "SSucUeB4ulB9",
        "outputId": "5d903632-e879-4c3c-f0fa-f4f21400ac8e",
        "execution": {
          "iopub.status.busy": "2023-11-21T19:02:04.670561Z",
          "iopub.execute_input": "2023-11-21T19:02:04.671613Z",
          "iopub.status.idle": "2023-11-21T19:02:04.683673Z",
          "shell.execute_reply.started": "2023-11-21T19:02:04.671575Z",
          "shell.execute_reply": "2023-11-21T19:02:04.682602Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table style=\"border:1px solid black\" >\n",
              "  <tr>\n",
              "    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n",
              "    <th style=\"text-align: center; border:1px solid black\">BEFORE</th>\n",
              "    <th style=\"text-align: center; border:1px solid black\">AFTER</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">``</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">SequenCell® DNA, RNA & ChIP-seq Data Analysis Solution Now Available\n",
              "Pleasanton, California, U.S.A., November 19, 2018 – With the availability of the SequenCell® Software Suite version 7.4, scientists can now access the latest version of the software for data analysis in their research. Scientists and lab staff can now evaluate all available information while using the most up to</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">Multimedia is one of the major components in a modern web design trend. In websites where you would like to make sure that your users are in for not only browsing but also have a good story telling with your website, using multimedia becomes an obvious choice.\n",
              "The good news is that creating a Multimedia site is not as hard and cumbersome as it used to be. Webs is a perfect example on how a non-technical user can use the features offered by its platform</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`import`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">import re\n",
              "from twisted. Triplet import Property\n",
              "from twisted.words import spaces\n",
              "from twisted.internet import reactor\n",
              "\n",
              "\n",
              "from .util import make_properties\n",
              "\n",
              "\n",
              "class PropertyList(Triplet):\n",
              "    \"\"\"\n",
              "    Inspired by http://twistedmatrix.com/trac/tags/latest/trunk/twisted/triplet/Triplet/\n",
              "    \"\"\"\n",
              "    def __init__(self</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">import inspect\n",
              "\n",
              "import nose.Case\n",
              "from _pytest.fixture import Nested\n",
              "\n",
              "def _test_to_native(cls):\n",
              "    \"\"\"Test that attributes/methods are converted to native\"\"\"\n",
              "    for name, value in inspect.getmembers(cls):\n",
              "        if inspect.isfunction(value):\n",
              "            yield _function_to_native, name, value\n",
              "        else:\n",
              "            if issubclass(value, inspect):\n",
              "               </pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`from`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">from _frolic.app import App\n",
              "from _frolic.util import create_app\n",
              "\n",
              "\n",
              "if __name__ == '__main__':\n",
              "    from _frolic.app import web\n",
              "    from _frolic.util import create_app2\n",
              "    create_app2()\n",
              "    from _frolic.app import wsgi\n",
              "    from _frolic.util import create_app3\n",
              "    create_app3(</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">from __future__ import absolute_import, division, print_function\n",
              "try:\n",
              "  from . import _io\n",
              "finally:\n",
              "  pass\n",
              "\n",
              "from . import _io\n",
              "from . import _regression\n",
              "\n",
              "import gc\n",
              "import os\n",
              "\n",
              "\n",
              "class IO(object):\n",
              "\n",
              "  def __init__(self):\n",
              "    self._io = _io\n",
              "    self._regression = _regression\n",
              "\n",
              "  def load(self, filepath</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`while`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">while both the Samsung Galaxy J5 and the LG Leon 4G are mid-range phones, the former’s specs definitely trump those of the latter. The Samsung Galaxy J5 has a 5 inch full HD display while the Leon 4G sports a 4.5 inch WVGA unit. The former’s got a 1.5GHz 64 bit Snapdragon 410 processor while</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">while both of us were away on holiday\n",
              "I wrote a rather long post about this in January last year, if you’re curious.\n",
              "There is an elephant in the room. A very big elephant. And our family is going to see if we can move it or not.\n",
              "I’m not going to name the elephant, because it really is our choice whether we want to take it on, and I want to make sure there’s room</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`try`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">tryingtothrive.com\n",
              "Trying To Thrive 30 Day Challenge For Beginners: Day 20 How You Can Try The 30 Day Challenge For Beginners: Day 20\n",
              "There are times in any person’s life that are meant for special celebrations, and you and I are aware of the fact that there is a celebration almost every weekend.\n",
              "That is a natural thing because we tend to look out for anything that comes our way</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">try them online by logging in at the normal time (e.g. 6pm on Friday) and if there are places available you will be sent an automatically generated email to your registration email address on the same evening notifying you of your booking. This will include the instructions for downloading the required files onto your mp3 player in time for your start location on the first day.\n",
              "Please read our terms and conditions page and our data protection policy page.\n",
              "If you still want to</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`if`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\"> if you want to receive an offer!\n",
              "By completing the survey, you will receive an offer for a 5 minutes online assessment, you will be able to see if you match the above profile and you might receive a second chance in an interview.</s></pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">if you’re reading this you’re gonna be one\n",
              "of those guys in your 80s and 90s\n",
              "with a great story to tell\n",
              "about the time you heard the Beach Boys play\n",
              "back when they were still cool enough for him to take you to\n",
              "he must have thought\n",
              "that his kids wouldn’t care\n",
              "that no one else would care\n",
              "and the kids grew up to be teenagers\n",
              "but no matter because\n",
              "your</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`for`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">for $150 each.  We also have three 1998 A185s, $1000 each.\n",
              "I'll keep an eye out for good stuff while I'm there :)\n",
              "I'm leaving for the south of France on Thursday.\n",
              "I have a pair of original KEF 115/2's (A12) for sale.\n",
              "I'm thinking either $1350 or</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">for all the children I'll never have!\n",
              "Mom's been babysitting for the kids across the street for the entire time we've lived here (going on 3.5 years!). She would get us 2 hours at a time every few weeks or so throughout that time. I've been dying to have a few of those times to take care of my own kiddos. My two brothers have 6 kids each (yes,</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`torch`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">torch-singer\n",
              "n (US, informal) someone who sings, usually poorly, in a nightclub or hotel lounge, etc., after the closing of the performance or in between the performers\n",
              "The torch-singer, like the balladeer, is sometimes thought of as being on the bottom, as opposed to rising up the ranks\n",
              "Sourcetorch-singer on Thesaurus\n",
              "v.24.1\n",
              "</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">torchlite.com\n",
              "1985—1992 • 1992—2010 • 2010— Present\n",
              "We design, we produce, we deliver.\n",
              "We’re experienced and creative with what-if’s and “how-can-I’s.”\n",
              "We use sustainable fabrics and materials.\n",
              "And we listen to what has been missing all along — the voices of women</pre></td>\n",
              "  </tr>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you reach this: congratulations! you've completed everything in this practice session.\n",
        "\n",
        "If you want to dig deeper, try to implement prompt-tuning (for bonus points!).\n",
        "You can read more about prompt tuning variants in paper [1](https://arxiv.org/abs/2104.08691) or paper [2](https://arxiv.org/abs/2101.00190). Both versions can be implemented by passing trainable prompts as `model.forward(..., past_key_values=your_prompts)`.\n",
        "\n",
        "\n",
        "\n",
        "### Read more\n",
        "\n",
        "* How post-training quantization works: https://arxiv.org/abs/2208.07339\n",
        "* An overview of running large models: https://huggingface.co/docs/accelerate/package_reference/big_modeling\n",
        "* A general library for different adapter types: https://adapterhub.ml/\n",
        "\n",
        "\n",
        "### [extra info] Running other models.\n",
        "\n",
        "This notebook's code can run with other models of similar size, such as [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b), [OPT-6.7B](https://huggingface.co/facebook/opt-6.7b) or [BLOOM-7.1B](https://huggingface.co/bigscience/bloom-7b1). However, they will require minor code tweaks:\n",
        "1. change the model name in `AutoModelForCausalLM.from_pretrained()` __and__ `AutoTokenizer`\n",
        "2. In the prompt tuning code, change `model.model.embed_tokens` to refer to the target model's word embeddings. Simply `print(model)` to navigate to them.\n",
        "3. Change code to add Lora layers - specifically where you what the transformer block components, since those components now have different names."
      ],
      "metadata": {
        "id": "hrKidv5KulB9"
      }
    }
  ]
}