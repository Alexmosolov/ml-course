Attention basics and Tensorboard example:
* Self-practice version [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/girafe-ai/ml-course/blob/24s_harbour_dlia/day03_attention/practice_Attention_basics_and_tensorboard.ipynb)

* Solved version [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/girafe-ai/ml-course/blob/24s_harbour_dlia/day03_attention/practice_Attention_basics_and_tensorboard__solved.ipynb)

Understanding the positional encoding:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/girafe-ai/ml-course/blob/24s_harbour_dlia/day03_attention/practice_positional_encoding.ipynb)



Further readings:

- Great blog post by Jay Alammar:
  https://jalammar.github.io/illustrated-transformer/
