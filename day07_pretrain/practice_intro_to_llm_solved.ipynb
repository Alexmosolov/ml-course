{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSWEcS2XKgzi"
      },
      "source": [
        "### Practice 07: Large Language Models and Their Implications\n",
        "\n",
        "The notebook is based on seminar from [YSDA NLP course](https://github.com/yandexdataschool/nlp_course/blob/2023/week06_llm/practice.ipynb).\n",
        "\n",
        "Also many thanks to works of: Tim Dettmers, Ruslan Svirschevsky, Artem Chumachenko, Younes Belkada, Felix Marty, Yulian Gilyazev, Gosha Zolotov, Andrey Ishutin,  Elena Volf, Artemiy Vishnyakov, Svetlana Shirokovskih."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKw-mjuRKgzs"
      },
      "source": [
        "### Local inference\n",
        "\n",
        "Now, let's try and load a model that can fit a typical Colab GPU (T4 with 16 GB as of spring 2023).\n",
        "\n",
        "Our best candidates are the smaller versions of the best performing open source models:\n",
        "- 7B parameters version of [LLaMA](https://arxiv.org/pdf/2302.13971.pdf) - best for spring 2023, released by Facebook\n",
        "- 7B parameters version of [Falcon](https://falconllm.tii.ae) - close competitor to Llama, released in May 2023 by [Technology Innovation Institute of UAE](https://www.tii.ae).\n",
        "- 6.7B parameters version of [OPT](https://arxiv.org/abs/2205.01068) - top choice in this nomination in 2022, released by Facebook.\n",
        "\n",
        "Beware: while these models are smaller than the ones in API, they're still over 60x larger than the BERT we played with last time. The code below will *just barely* fit into memory, so make sure you don't have anything else loaded. Sometimes you may need to restart runtime for the code to work.\n",
        "\n",
        "It's a good time to restart your kernel and switch to GPU! (Runtime -> Change runtime type)\n",
        "<center><img src=\"https://i.imgur.com/OOfDYzJ.png\" width=240px></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xeRF_hSKgzs"
      },
      "outputs": [],
      "source": [
        "%pip install --quiet bitsandbytes==0.41.1 transformers==4.34.1 accelerate==0.24.0 sentencepiece==0.1.99 optimum==1.13.2 auto-gptq==0.4.2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "import bitsandbytes as bnb\n",
        "from tqdm.auto import tqdm, trange\n",
        "assert torch.cuda.is_available(), \"you need cuda for this part\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "from joblib import Parallel, delayed\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMzFwx29Kgzu"
      },
      "outputs": [],
      "source": [
        "model_name = 'TheBloke/Llama-2-13B-GPTQ'\n",
        "\n",
        "# loading Llama tokenizer ...\n",
        "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# ... and the model itself\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map='auto',\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    offload_state_dict=True\n",
        ")\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k2zCgAhG7l5"
      },
      "source": [
        "## Text generation\n",
        "\n",
        "**Comparison of strategies for language model text generation:**\n",
        "\n",
        "| Strategy | Description | Pros & Cons |\n",
        "| --- | --- | --- |\n",
        "| Greedy Search | Chooses the word with the highest probability as the next word in the sequence. | **Pros:** Simple and fast. <br> **Cons:** Can lead to repetitive and incoherent text. |\n",
        "| Sampling with Temperature | Introduces randomness in the word selection. A higher temperature leads to more randomness. | **Pros:** Allows exploration and diverse output. <br> **Cons:** Higher temperatures can lead to nonsensical outputs. |\n",
        "| Nucleus Sampling (Top-p Sampling) | Selects the next word from a truncated vocabulary, the \"nucleus\" of words that have a cumulative probability exceeding a pre-specified threshold (p). | **Pros:** Balances diversity and quality. <br> **Cons:** Setting an optimal 'p' can be tricky. |\n",
        "| Beam Search | Explores multiple hypotheses (sequences of words) at each step, and keeps the 'k' most likely, where 'k' is the beam width. | **Pros:** Produces more reliable results than greedy search. <br> **Cons:** Can lack diversity and lead to generic responses. |\n",
        "| Top-k Sampling | Randomly selects the next word from the top 'k' words with the highest probabilities. | **Pros:** Introduces randomness, increasing output diversity. <br> **Cons:** Random selection can sometimes lead to less coherent outputs. |\n",
        "| Length Normalization | Prevents the model from favoring shorter sequences by dividing the log probabilities by the sequence length raised to some power. | **Pros:** Makes longer and potentially more informative sequences more likely. <br> **Cons:** Tuning the normalization factor can be difficult. |\n",
        "| Stochastic Beam Search | Introduces randomness into the selection process of the 'k' hypotheses in beam search. | **Pros:** Increases diversity in the generated text. <br> **Cons:** The trade-off between diversity and quality can be tricky to manage. |\n",
        "| Decoding with Minimum Bayes Risk (MBR) | Chooses the hypothesis (out of many) that minimizes expected loss under a loss function. | **Pros:** Optimizes the output according to a specific loss function. <br> **Cons:** Computationally more complex and requires a good loss function. |\n",
        "\n",
        "Documentation references:\n",
        "- [reference for `AutoModelForCausalLM.generate()`](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationMixin.generate)\n",
        "- [reference for `AutoTokenizer.decode()`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode)\n",
        "- Huggingface [docs on generation strategies](https://huggingface.co/docs/transformers/generation_strategies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWm6KDSzMiAf"
      },
      "source": [
        "### Generation with HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGfyeM-vdq5o",
        "outputId": "e6e94e18-7669-448e-c289-ac3ded036b0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input batch (encoded): {'input_ids': tensor([[    1,   450,   937, 10943, 14436,   713,  2834,   689,  3430,   763]],\n",
            "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Output: <s>The first discovered martian lifeform looks like an eyeless bird-pig thing\n",
            "And it’s the size of a golf ball.\n",
            "The Martian surface is home to a bumper crop of bacteria that use iron to stay alive.\n",
            "Image: NASA/JPL-Caltech/MSSS\n",
            "We finally have\n"
          ]
        }
      ],
      "source": [
        "prompt = 'The first discovered martian lifeform looks like'\n",
        "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "print(\"Input batch (encoded):\", batch)\n",
        "\n",
        "output_tokens = model.generate(**batch, max_new_tokens=64, do_sample=True, temperature=0.8)\n",
        "# greedy inference:                                        do_sample=False)\n",
        "# beam search for highest probability:                     num_beams=4)\n",
        "\n",
        "print(\"\\nOutput:\", tokenizer.decode(output_tokens[0].cpu()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P17ehC1sKgzx"
      },
      "source": [
        "#### Low-level code for text generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZJvOMbmG7l8",
        "outputId": "9fb44fd0-eaaa-4331-8113-c413848cd63b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moscow is the capital of \n",
            "\n",
            "Step #0 candidates:\n",
            "▁Russia   : 0.7616 \n",
            "▁the      : 0.1795 \n",
            "▁Russian  : 0.0218 \n",
            "▁a        : 0.0058 \n",
            "▁not      : 0.0022 \n",
            "\n",
            "Chosen token: Russia\n",
            "\n",
            "Step #1 candidates:\n",
            ".         : 0.3238 \n",
            ",         : 0.3188 \n",
            "▁and      : 0.1845 \n",
            "and       : 0.0554 \n",
            "<0x0A>    : 0.0080 \n",
            "\n",
            "Chosen token: where\n",
            "\n",
            "Step #2 candidates:\n",
            "▁it       : 0.2687 \n",
            "▁the      : 0.2245 \n",
            "▁you      : 0.0217 \n",
            "▁there    : 0.0209 \n",
            "▁most     : 0.0207 \n",
            "\n",
            "Chosen token: it\n",
            "\n",
            "Step #3 candidates:\n",
            "▁is       : 0.5815 \n",
            "▁has      : 0.0585 \n",
            "’         : 0.0320 \n",
            "'         : 0.0196 \n",
            "▁serves   : 0.0165 \n",
            "\n",
            "Chosen token: is\n",
            "\n",
            "Step #4 candidates:\n",
            "▁located  : 0.1844 \n",
            "▁the      : 0.1287 \n",
            "located   : 0.1267 \n",
            "▁also     : 0.0995 \n",
            "▁situated : 0.0927 \n",
            "\n",
            "Chosen token: Home\n",
            "\n",
            "Step #5 candidates:\n",
            "▁to       : 0.7887 \n",
            "▁of       : 0.0769 \n",
            "▁for      : 0.0323 \n",
            "▁         : 0.0071 \n",
            "▁To       : 0.0055 \n",
            "\n",
            "Chosen token: to\n",
            "\n",
            "Step #6 candidates:\n",
            "▁the      : 0.2330 \n",
            "▁         : 0.1843 \n",
            "▁more     : 0.0700 \n",
            "▁over     : 0.0369 \n",
            "▁a        : 0.0240 \n",
            "\n",
            "Chosen token: much\n",
            "\n",
            "Step #7 candidates:\n",
            "▁of       : 0.5137 \n",
            "of        : 0.1449 \n",
            "▁more     : 0.1004 \n",
            "more      : 0.0258 \n",
            "o         : 0.0217 \n",
            "\n",
            "Chosen token: of\n",
            "\n",
            "Step #8 candidates:\n",
            "▁the      : 0.2632 \n",
            "Russ      : 0.2066 \n",
            "the       : 0.0917 \n",
            "▁its      : 0.0351 \n",
            "its       : 0.0263 \n",
            "\n",
            "Chosen token: Europe\n",
            "\n",
            "Step #9 candidates:\n",
            "an        : 0.2085 \n",
            "’         : 0.1974 \n",
            "'         : 0.1125 \n",
            ".         : 0.1057 \n",
            "and       : 0.0791 \n",
            "\n",
            "Chosen token: ’\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt1 = \"Moscow is the capital of\"\n",
        "# prompt1 = \"Skippy, a young android, likes to dream about electric\"\n",
        "\n",
        "print(prompt1, '\\n')\n",
        "\n",
        "voc = tokenizer.get_vocab()\n",
        "voc_rev = {v:k for k, v in voc.items()}  # reverse vocab for decode\n",
        "past_key_values = None\n",
        "\n",
        "for i in range(10):\n",
        "    batch1 = tokenizer(prompt1, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "    logits = model.forward(**batch1).logits[0, -1, :]\n",
        "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    next_token_id = torch.multinomial(probs.flatten(), num_samples=1)\n",
        "\n",
        "    next_token = tokenizer.decode(next_token_id)\n",
        "    prompt1 += next_token\n",
        "\n",
        "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "    top_tokens = sorted_indices[:5]\n",
        "    print(f\"Step #{i} candidates:\")\n",
        "    for t, p in zip (top_tokens, sorted_probs):\n",
        "        t = voc_rev[t.item()]\n",
        "        print(f\"{t:<10}: {p:.4f} \")\n",
        "\n",
        "    print(f'\\nChosen token: {next_token}', end='\\n\\n', flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a686Z2SQKgz0"
      },
      "source": [
        "**Nucleus sampling generation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WIqDgfBKgz0"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, List\n",
        "def nucleus_sampling(model, tokenizer, prompt: str, prob: float = 0.5) -> Tuple[str, List[str]]:\n",
        "    \"\"\"generates the next token from the nucleus of tokens with cumulative probability up to param:prob\"\"\"\n",
        "\n",
        "    batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "    logits = model.forward(**batch).logits[0, -1, :]\n",
        "    probs = torch.nn.functional.softmax(logits, dim=-1).flatten()\n",
        "    token_inds = torch.argsort(probs)\n",
        "\n",
        "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "\n",
        "    probs_sum = torch.cumsum(sorted_probs, dim=0)\n",
        "    probs_last_nucleus = torch.nonzero(probs_sum>prob)[0]\n",
        "\n",
        "    sorted_probs_sum = sorted_probs[probs_last_nucleus]\n",
        "    sorted_probs = sorted_probs[:probs_last_nucleus]\n",
        "    sorted_indices = sorted_indices[:probs_last_nucleus]\n",
        "\n",
        "    sorted_probs /= sorted_probs[probs_last_nucleus-1]\n",
        "\n",
        "    sampled_token_id = torch.multinomial(sorted_probs, num_samples=1)\n",
        "    sampled_token = tokenizer.decode(sorted_indices[sampled_token_id])\n",
        "\n",
        "    possible_tokens = [tokenizer.decode(token_id) for token_id in sorted_indices]\n",
        "\n",
        "    # sampled_token should be a string token that was generated\n",
        "    # possible_tokens should be a list of all tokens that have non-zero probability\n",
        "    return sampled_token, possible_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzPLQl-HKgz1",
        "outputId": "c0d5732c-06a1-4453-8e7c-5cf12a63ec5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Elbrus is the highest peak ['peak', 'mountain', 'point']\n",
            "Large language models can learn to do ['generate', 'write', 'perform', 'do', 'speak', 'be', 'predict', 'communicate']\n"
          ]
        }
      ],
      "source": [
        "# Tests for nucleus sampling\n",
        "test_prompt = \"Elbrus is the highest\"\n",
        "next_token, possible_tokens = nucleus_sampling(model, tokenizer, test_prompt, prob=0.9)\n",
        "print(test_prompt, next_token, possible_tokens)\n",
        "assert next_token in possible_tokens\n",
        "assert 3 <= len(possible_tokens) <= 3\n",
        "assert sorted(possible_tokens) == ['mountain', 'peak', 'point']\n",
        "\n",
        "test_prompt = \"Large language models can learn to\"\n",
        "next_token, possible_tokens = nucleus_sampling(model, tokenizer, test_prompt, prob=0.4)\n",
        "print(test_prompt, next_token, possible_tokens)\n",
        "assert next_token in possible_tokens\n",
        "assert sorted(possible_tokens) == ['be', 'communicate', 'do', 'generate', 'perform', 'predict', 'speak', 'write']\n",
        "assert len(possible_tokens) == 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZaQZhPXOPSG"
      },
      "source": [
        "### Part 3: Chain-of-thought prompting\n",
        "\n",
        "![img](https://github.com/kojima-takeshi188/zero_shot_cot/raw/main/img/image_stepbystep.png)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2AmfelTn5en",
        "outputId": "e9a40f6e-50aa-41bf-d8e0-eacecd05fc2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-11-02 17:32:45--  https://raw.githubusercontent.com/kojima-takeshi188/zero_shot_cot/2824685e25809779dbd36900a69825068e9f51ef/dataset/AQuA/test.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 130192 (127K) [text/plain]\n",
            "Saving to: ‘aqua.json’\n",
            "\n",
            "aqua.json           100%[===================>] 127.14K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-11-02 17:32:46 (11.6 MB/s) - ‘aqua.json’ saved [130192/130192]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "import locale; locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!wget https://raw.githubusercontent.com/kojima-takeshi188/zero_shot_cot/2824685e25809779dbd36900a69825068e9f51ef/dataset/AQuA/test.json -O aqua.json\n",
        "data = list(map(json.loads, open(\"aqua.json\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IATXmPfYw8s6",
        "outputId": "6688758d-83b6-4552-a3e8-2bbe352249fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'question': 'Janice bikes at 10 miles per hour, while Jennie bikes at 20. How long until they have collectively biked 1 mile?',\n",
              " 'options': ['A)1 minute',\n",
              "  'B)2 minutes',\n",
              "  'C)3 minutes',\n",
              "  'D)4 minutes',\n",
              "  'E)5 minutes'],\n",
              " 'rationale': \"Janice's speed = 1/6 miles per minute\\nJennie's speed = 1/3 miles per minute\\nJanice + Jennie's speed= (1/6 + 1/3) = 1/2 miles per minute\\nBoth together will finish the mile in 2 minutes\\ncorrect option is B\",\n",
              " 'correct': 'B'}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Example:\")\n",
        "data[150]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UcOYQPW8sVq"
      },
      "source": [
        "### Naive solution\n",
        "\n",
        "Here, we prompt the model to choose an answer to the example above (`data[150]`) out of the options given above. We're using a format that mimics grade school solution textbook.\n",
        "\n",
        "Please note that there are minor formatting changes in options: an extra space and an opening bracket. Those may or may not be important :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtkkdiJl3-UI"
      },
      "outputs": [],
      "source": [
        "EXAMPLE_0SHOT = \"\"\"\n",
        "Question: Janice bikes at 10 miles per hour, while Jennie bikes at 20. How long until they have collectively biked 1 mile?\n",
        "Answer Choices: (A) 1 minute (B) 2 minutes (C) 3 minutes (D) 4 minutes (E) 5 minutes\n",
        "Correct Answer:\n",
        "\"\"\".strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyQ_8tJc6nyv",
        "outputId": "5666c3b1-aff2-4b5e-e118-136fe2c4e841"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Prompt:]\n",
            "Question: Janice bikes at 10 miles per hour, while Jennie bikes at 20. How long until they have collectively biked 1 mile?\n",
            "Answer Choices: (A) 1 minute (B) 2 minutes (C) 3 minutes (D) 4 minutes (E) 5 minutes\n",
            "Correct Answer:\n",
            "================================================================================\n",
            "[Generated:] (E) 5 minutes\n",
            "Explanation: Jennie bikes at 20 miles per hour for 2 minutes. She will have travelled 2 miles in this time. Janice also bikes for 2 minutes, but at a slower speed of 10 miles per hour. This means that she will travel 2/10 miles or 0.2 miles. She will travel 1 mile in 5 minutes. Hence, 5 minutes will have el\n"
          ]
        }
      ],
      "source": [
        "# solving an equation directly\n",
        "batch = tokenizer(EXAMPLE_0SHOT, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "torch.manual_seed(1337)\n",
        "output_tokens = model.generate(**batch, max_new_tokens=100, do_sample=True, top_p=0.9)\n",
        "print(\"[Prompt:]\\n\" + EXAMPLE_0SHOT)\n",
        "print(\"=\" * 80)\n",
        "print(\"[Generated:]\", tokenizer.decode(output_tokens[0][batch['input_ids'].shape[1]:].cpu()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suSkiDk28I6C"
      },
      "source": [
        "And here's how you can solve this with few-shot chain-of-thought prompting.\n",
        "\n",
        "You need to chang 3 things\n",
        "- use a new field called **Rationale**, that contains a step-by-step solution to the problem\n",
        "- add several few-shot examples of previously solved problems **with rationales**\n",
        "- change the final prompt so that the model has to generate rationale before answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0F1jYdRvoJW"
      },
      "outputs": [],
      "source": [
        "EXAMPLE_3SHOT_CHAIN_OF_THOUGHT = \"\"\"\n",
        "Question: The original retail price of an appliance was 60 percent more than its wholesale cost. If the appliance was actually sold for 20 percent less than the original retail price, then it was sold for what percent more than its wholesale cost?\n",
        "Answer Choices: (A) 20% (B) 28% (C) 36% (D) 40% (E) 42%\n",
        "Rationale: wholesale cost = 100;\\noriginal price = 100*1.6 = 160;\\nactual price = 160*0.8 = 128.\\nAnswer: B.\n",
        "Correct Answer: B\n",
        "\n",
        "\n",
        "Question: A grocer makes a 25% profit on the selling price for each bag of flour it sells. If he sells each bag for $100 and makes $3,000 in profit, how many bags did he sell?\n",
        "Answer Choices: (A) 12 (B) 16 (C) 24 (D) 30 (E) 40\n",
        "Rationale: Profit on one bag: 100*1.25= 125\\nNumber of bags sold = 3000/125 = 24\\nAnswer is C.\n",
        "Correct Answer: C\n",
        "\n",
        "\n",
        "Question: 20 marbles were pulled out of a bag of only white marbles, painted black, and then put back in. Then, another 20 marbles were pulled out, of which 1 was black, after which they were all returned to the bag. If the percentage of black marbles pulled out the second time represents their percentage in the bag, how many marbles in total Q does the bag currently hold?\n",
        "Answer Choices: (A) 40 (B) 200 (C) 380 (D) 400 (E) 3200\n",
        "Rationale: We know that there are 20 black marbles in the bag and this number represent 1/20 th of the number of all marbles in the bag, thus there are total Q of 20*20=400 marbles.\\nAnswer: D.\n",
        "Correct Answer: D\n",
        "\n",
        "\n",
        "Question: Janice bikes at 10 miles per hour, while Jennie bikes at 20. How long until they have collectively biked 1 mile?\n",
        "Answer Choices: (A) 1 minute (B) 2 minutes (C) 3 minutes (D) 4 minutes (E) 5 minutes\n",
        "Rationale:\n",
        "\"\"\".strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tn8QoAYcRkHC",
        "outputId": "cefbed8a-b786-40a0-88fd-cbbfc38c94fe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Prompt:]\n",
            "Question: The original retail price of an appliance was 60 percent more than its wholesale cost. If the appliance was actually sold for 20 percent less than the original retail price, then it was sold for what percent more than its wholesale cost?\n",
            "Answer Choices: (A) 20% (B) 28% (C) 36% (D) 40% (E) 42%\n",
            "Rationale: wholesale cost = 100;\n",
            "original price = 100*1.6 = 160;\n",
            "actual price = 160*0.8 = 128.\n",
            "Answer: B.\n",
            "Correct Answer: B\n",
            "\n",
            "\n",
            "Question: A grocer makes a 25% profit on the selling price for each bag of flour it sells. If he sells each bag for $100 and makes $3,000 in profit, how many bags did he sell?\n",
            "Answer Choices: (A) 12 (B) 16 (C) 24 (D) 30 (E) 40\n",
            "Rationale: Profit on one bag: 100*1.25= 125\n",
            "Number of bags sold = 3000/125 = 24\n",
            "Answer is C.\n",
            "Correct Answer: C\n",
            "\n",
            "\n",
            "Question: 20 marbles were pulled out of a bag of only white marbles, painted black, and then put back in. Then, another 20 marbles were pulled out, of which 1 was black, after which they were all returned to the bag. If the percentage of black marbles pulled out the second time represents their percentage in the bag, how many marbles in total Q does the bag currently hold?\n",
            "Answer Choices: (A) 40 (B) 200 (C) 380 (D) 400 (E) 3200\n",
            "Rationale: We know that there are 20 black marbles in the bag and this number represent 1/20 th of the number of all marbles in the bag, thus there are total Q of 20*20=400 marbles.\n",
            "Answer: D.\n",
            "Correct Answer: D\n",
            "\n",
            "\n",
            "Question: Janice bikes at 10 miles per hour, while Jennie bikes at 20. How long until they have collectively biked 1 mile?\n",
            "Answer Choices: (A) 1 minute (B) 2 minutes (C) 3 minutes (D) 4 minutes (E) 5 minutes\n",
            "Rationale:\n",
            "================================================================================\n",
            "[Generated:] 10 + 20 = 30 miles per hour, thus the time required for them to bike 1 mile collectively is 1/30th of an hour, which is 1/30th of 60= 2 minutes\n",
            "Answer is B.\n",
            "Correct Answer: B\n",
            "\n",
            "Question: How many different times tables are there in the range of 10 times 10 and 20 times 20?\n",
            "Answer\n"
          ]
        }
      ],
      "source": [
        "batch = tokenizer(EXAMPLE_3SHOT_CHAIN_OF_THOUGHT, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "torch.manual_seed(1337)\n",
        "output_tokens = model.generate(**batch, max_new_tokens=100, do_sample=True, top_p=0.9)\n",
        "print(\"[Prompt:]\\n\" + EXAMPLE_3SHOT_CHAIN_OF_THOUGHT)\n",
        "print(\"=\" * 80)\n",
        "print(\"[Generated:]\", tokenizer.decode(output_tokens[0][batch['input_ids'].shape[1]:].cpu()))\n",
        "#### NOTE: scroll down for the final answer (below the ======= line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4px3jv-99-m"
      },
      "source": [
        "Write a function that automatically creates chain-of-thought prompts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ntyFPMt9fyt",
        "outputId": "6a4be50d-004a-4c84-c198-aef10459e1bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Well done!\n"
          ]
        }
      ],
      "source": [
        "QUESTION_PREFIX = \"Question: \"\n",
        "OPTIONS_PREFIX = \"Answer Choices: \"\n",
        "CHAIN_OF_THOUGHT_PREFIX = \"Rationale: \"\n",
        "ANSWER_PREFIX = \"Correct Answer: \"\n",
        "FEWSHOT_SEPARATOR = \"\\n\\n\\n\"\n",
        "\n",
        "def process_single_prompt(data, is_main=False, join_with=\"\\n\"):\n",
        "    to_join = []\n",
        "    to_join.append(QUESTION_PREFIX+data[\"question\"].strip())\n",
        "    options = [option.split(\")\",1) for option in data[\"options\"]]\n",
        "    to_join.append(OPTIONS_PREFIX+\" \".join([f\"({name.strip()}) {val.strip()}\" for name,val in options]))\n",
        "    if(not is_main):\n",
        "        to_join.append(CHAIN_OF_THOUGHT_PREFIX+data[\"rationale\"].strip())\n",
        "        to_join.append(ANSWER_PREFIX+data[\"correct\"].strip())\n",
        "    else:\n",
        "        to_join.append(CHAIN_OF_THOUGHT_PREFIX.strip())\n",
        "\n",
        "    return join_with.join(to_join)\n",
        "\n",
        "\n",
        "def make_prompt(*, main_question, fewshot_examples, process_func=process_single_prompt):\n",
        "  \"\"\"\n",
        "  Your goal is to produce the same prompt as the EXAMPLE_3SHOT_CHAIN_OF_THOUGHT automatically\n",
        "\n",
        "  For each few-shot question, make sure to follow the following rules:\n",
        "  1. Each question begins with QUESTION_PREFIX, after which you should print the question without leading/traiiling spaces (if any)\n",
        "  2. After the question, provide space-separated options. Each option should be put in double brackets, followed by option text, e.g. \"(A) 146%\"\n",
        "  3. Then, provide the answer as a single letter (A-E)\n",
        "  4. Finally, add trailing newlines from FEWSHOT_SEPARATOR\n",
        "\n",
        "  Your final prompt should contain all fewshot_examples (in order), separated with FEWSHOT_SEPARATOR, then follow with main_question.\n",
        "  The main_question should contain the question and options formatted the same way as in FEWSHOT_EXAMPLES.\n",
        "  After that, you should prompt the model to produce an explanation (rationale) for the answer.\n",
        "\n",
        "  Please make sure your prompt contains no leading/trailing newlines or spaces, same as in EXAMPLE_3SHOT_CHAIN_OF_THOUGHT\n",
        "  \"\"\"\n",
        "\n",
        "  return FEWSHOT_SEPARATOR.join([process_func(fe) for fe in fewshot_examples]+[process_func(main_question, True)])\n",
        "\n",
        "\n",
        "generated_fewshot_prompt = make_prompt(main_question=data[150], fewshot_examples=(data[30], data[20], data[5]))\n",
        "assert generated_fewshot_prompt == EXAMPLE_3SHOT_CHAIN_OF_THOUGHT, \"prompts don't match\"\n",
        "assert generated_fewshot_prompt != make_prompt(main_question=data[150], fewshot_examples=())\n",
        "assert generated_fewshot_prompt.endswith(make_prompt(main_question=data[150], fewshot_examples=()))\n",
        "\n",
        "print(\"Well done!\")\n",
        "\n",
        "# Hint: if two prompts do not match, you may find it usefull to use https://www.diffchecker.com or similar to find the difference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7DzQ8hfOcFR"
      },
      "source": [
        "**Evaluate your prompt.**\n",
        "\n",
        "Please run the model on the entire dataset and measure it's accuracy.\n",
        "For each question, peak $n=5$ other questions at random to serve as few-shot examples. Make sure not to accidentally sample the main_question among few-shot examples. For scientific evaluation, it is also a good practice to split the data into two parts: one for eval, and another for few-shot examples. However, doing so is optional in this homework.\n",
        "\n",
        "The tricky part is when to stop generating: if you don't control for this, your model can accidentally generate a whole new question - and promptyly answer it :) To make sure you get the correct answer, stop generating tokens when the model is done explaining it's solution. To circumvent this, you need to __stop generating as soon as the model generates Final Answer: [A-E]__\n",
        "To do so, you can either generate manually (see low-level generation above) or use [transformers stopping criteria](https://discuss.huggingface.co/t/implimentation-of-stopping-criteria-list/20040/2), whichever you prefer.\n",
        "\n",
        "If you do everything right, the model should be much better than random. However, please __do not expect miracles__: this is far from the best models, and it will perform much worse than an average human."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMvj9eCtQwnz"
      },
      "outputs": [],
      "source": [
        "NUM_SAMPLES = 0    # use this to count how many samples you evaluated\n",
        "NUM_RESPONDED = 0  # how many times did the model produce Correct Answer: (letter) in it's response. use as a sanity check.\n",
        "NUM_CORRECT = 0    # how many times did the model's chosen answer (letter) match the correct answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGEYGzQccR2N"
      },
      "outputs": [],
      "source": [
        "class StoppingCriteriaSub(StoppingCriteria):\n",
        "    def __init__(self, stops = []):\n",
        "        super().__init__()\n",
        "        self.stops = [stop.to(\"cuda\") for stop in stops]\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
        "        for stop in self.stops:\n",
        "            if torch.all((stop == input_ids[0][-len(stop):])).item():\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "stop_words = [\"\\nQuestion:\"]\n",
        "stop_words_ids = [tokenizer(stop_word, return_tensors='pt')['input_ids'].squeeze()[2:] for stop_word in stop_words]\n",
        "stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVEznub9Kwyo"
      },
      "outputs": [],
      "source": [
        "def extract_single_answer(output_tokens):\n",
        "    ind = output_tokens.find(ANSWER_PREFIX)\n",
        "    if(ind==-1):\n",
        "        return -1\n",
        "    return output_tokens[ind+len(ANSWER_PREFIX)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "67503c805bb54f1487fb966665fbfac2",
            "4242032034bc4e6a86995f1a63764d59",
            "3206514edc6f41a09c3f21570acd2c94",
            "04b0581209564628a6020ba3beefb77b",
            "9fd5ea29bee14321875d35a2265e05c9",
            "afe44512f6c44e7884c0574b72bf6d01",
            "dfc2851f8a5c499eb8e58323612dc194",
            "570d3ab95c3c42d7a00459545a4555c7",
            "654d4c494bdb4b8e91060c5f3155e0fa",
            "4b5d109dfcf94eb8a142dffc8be4f9cb",
            "b5b78161e70d4e20b587cb53c271f87b"
          ]
        },
        "id": "2pfS1A1QQW8T",
        "outputId": "b4947070-e37c-4da2-fb34-05e79be80432"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67503c805bb54f1487fb966665fbfac2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/254 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def process_single_text(text_ind, shots_num=5, process_func=process_single_prompt):\n",
        "    fewshot_inds = np.random.choice(list(range(0,text_ind))+list(range(text_ind+1,len(data))), shots_num, replace=False)\n",
        "    fewshot_examples = [data[ind] for ind in fewshot_inds]\n",
        "    prompt = make_prompt(main_question=data[text_ind], fewshot_examples=fewshot_examples, process_func=process_func)\n",
        "    return tokenizer(prompt, return_tensors='pt', return_token_type_ids=False), data[text_ind][\"correct\"]\n",
        "\n",
        "data_processed = Parallel(n_jobs=os.cpu_count())(delayed(process_single_text)(text_ind) for text_ind in tqdm(range(len(data))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKtHeauUOg_5"
      },
      "outputs": [],
      "source": [
        "# Optionally, consider inferencing multiple sentences in a batch for faster inference;\n",
        "# If you choose to batch outputs, make sure the results are the same as with batch=1 (using greedy inference)\n",
        "# It throws CUDA OOM error when batch_size>1, so pipeline w/o batches (and w/o beams) is used\n",
        "def inference(data_processed, extract_fn=extract_single_answer):\n",
        "    global NUM_CORRECT, NUM_RESPONDED, NUM_SAMPLES\n",
        "    with torch.no_grad():\n",
        "        for idx, (tokens, labels) in enumerate(tqdm(data_processed)):\n",
        "            tokens = tokens.to(device)\n",
        "            output_tokens = model.generate(**tokens, max_new_tokens=1000, do_sample=True, top_p=0.9, temperature=0.8, stopping_criteria=stopping_criteria)\n",
        "            output_tokens_str = tokenizer.decode(output_tokens[0][tokens['input_ids'].shape[1]:].cpu())\n",
        "\n",
        "            answer = extract_fn(output_tokens_str)\n",
        "            NUM_CORRECT += np.sum(int((answer!=-1) & (answer==labels)))\n",
        "            NUM_RESPONDED += np.sum(answer!=-1)\n",
        "            NUM_SAMPLES += 1\n",
        "\n",
        "            if(idx%30==0):\n",
        "                print(f\"Processed {NUM_SAMPLES}:{NUM_CORRECT}/{NUM_RESPONDED}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "f0e6fb59440448bc9b4f2ac3f916a2b1",
            "5b4f326cfdd54513b9d4c5a77d9cbc30",
            "961dfecff4d94e0c90ae260aec15533c",
            "17c6ec7ef2264a11a075e0e4a537e7c3",
            "6bbf40f996684c8984c8c531e7d27571",
            "01ca1eb08840406b980037203fa39ffc",
            "7db7ff602111446eacf16d0da342135d",
            "52788bd86c904fe5844ab2a888bcb2f7",
            "4caefc4f714f4ccb9485774435188454",
            "8685f6bfb4304b4caeb36572afa29ea0",
            "5b9fceb7b4b64d958a18b181938a3c43"
          ]
        },
        "id": "Zh6gejr0JNuh",
        "outputId": "341678cf-ea3b-4ef9-bd48-2a0243b9f672"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0e6fb59440448bc9b4f2ac3f916a2b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/254 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 1:0/1\n",
            "Processed 31:16/30\n",
            "Processed 61:23/58\n",
            "Processed 91:31/87\n",
            "Processed 121:36/111\n",
            "Processed 151:44/140\n",
            "Processed 181:50/169\n",
            "Processed 211:57/198\n",
            "Processed 241:61/227\n"
          ]
        }
      ],
      "source": [
        "inference(data_processed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxrCJvxIJSjA",
        "outputId": "97552b3a-bff5-4d29-d65f-a32b992cb638"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Responded %%: 0.9448818897637795\n",
            "Accuracy (when responded): 0.2708333333333333\n",
            "Accuracy (overall): 0.2559055118110236\n"
          ]
        }
      ],
      "source": [
        "print(\"Responded %%:\", NUM_RESPONDED / NUM_SAMPLES)\n",
        "print(\"Accuracy (when responded):\", NUM_CORRECT / NUM_RESPONDED)\n",
        "print(\"Accuracy (overall):\", NUM_CORRECT / NUM_SAMPLES)\n",
        "\n",
        "if NUM_RESPONDED / NUM_SAMPLES < 0.9:\n",
        "    print(\"Something is wrong with the evaluation technique (for 5-shot CoT): the model refuses to answer too many questions.\")\n",
        "    print(\"Make sure you generate enough tokens that the model can produce a correct answer.\")\n",
        "    print(\"When in doubt, take a look at the full model output. You can often spot errors there.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9M4vhT5fCpEg"
      },
      "outputs": [],
      "source": [
        "history = [{\n",
        "    \"name\": \"baseline\",\n",
        "    \"responded\": NUM_RESPONDED / NUM_SAMPLES,\n",
        "    \"acc_resp\": NUM_CORRECT / NUM_RESPONDED,\n",
        "    \"acc_all\": NUM_CORRECT / NUM_SAMPLES\n",
        "}]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01ca1eb08840406b980037203fa39ffc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04b0581209564628a6020ba3beefb77b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b5d109dfcf94eb8a142dffc8be4f9cb",
            "placeholder": "​",
            "style": "IPY_MODEL_b5b78161e70d4e20b587cb53c271f87b",
            "value": " 254/254 [00:14&lt;00:00, 89.09it/s]"
          }
        },
        "17c6ec7ef2264a11a075e0e4a537e7c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8685f6bfb4304b4caeb36572afa29ea0",
            "placeholder": "​",
            "style": "IPY_MODEL_5b9fceb7b4b64d958a18b181938a3c43",
            "value": " 254/254 [53:58&lt;00:00,  7.09s/it]"
          }
        },
        "3206514edc6f41a09c3f21570acd2c94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_570d3ab95c3c42d7a00459545a4555c7",
            "max": 254,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_654d4c494bdb4b8e91060c5f3155e0fa",
            "value": 254
          }
        },
        "4242032034bc4e6a86995f1a63764d59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afe44512f6c44e7884c0574b72bf6d01",
            "placeholder": "​",
            "style": "IPY_MODEL_dfc2851f8a5c499eb8e58323612dc194",
            "value": "100%"
          }
        },
        "4b5d109dfcf94eb8a142dffc8be4f9cb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4caefc4f714f4ccb9485774435188454": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "52788bd86c904fe5844ab2a888bcb2f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "570d3ab95c3c42d7a00459545a4555c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b4f326cfdd54513b9d4c5a77d9cbc30": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01ca1eb08840406b980037203fa39ffc",
            "placeholder": "​",
            "style": "IPY_MODEL_7db7ff602111446eacf16d0da342135d",
            "value": "100%"
          }
        },
        "5b9fceb7b4b64d958a18b181938a3c43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "654d4c494bdb4b8e91060c5f3155e0fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "67503c805bb54f1487fb966665fbfac2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4242032034bc4e6a86995f1a63764d59",
              "IPY_MODEL_3206514edc6f41a09c3f21570acd2c94",
              "IPY_MODEL_04b0581209564628a6020ba3beefb77b"
            ],
            "layout": "IPY_MODEL_9fd5ea29bee14321875d35a2265e05c9"
          }
        },
        "6bbf40f996684c8984c8c531e7d27571": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7db7ff602111446eacf16d0da342135d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8685f6bfb4304b4caeb36572afa29ea0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "961dfecff4d94e0c90ae260aec15533c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52788bd86c904fe5844ab2a888bcb2f7",
            "max": 254,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4caefc4f714f4ccb9485774435188454",
            "value": 254
          }
        },
        "9fd5ea29bee14321875d35a2265e05c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afe44512f6c44e7884c0574b72bf6d01": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5b78161e70d4e20b587cb53c271f87b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dfc2851f8a5c499eb8e58323612dc194": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0e6fb59440448bc9b4f2ac3f916a2b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b4f326cfdd54513b9d4c5a77d9cbc30",
              "IPY_MODEL_961dfecff4d94e0c90ae260aec15533c",
              "IPY_MODEL_17c6ec7ef2264a11a075e0e4a537e7c3"
            ],
            "layout": "IPY_MODEL_6bbf40f996684c8984c8c531e7d27571"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}